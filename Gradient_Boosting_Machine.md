> 부스팅 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 </br>
> 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이다. </br>
> 부스팅의 대표적인 구현은 AdaBoost(Adaptive Boosting)와 그래디언트 부스트가 있다. </br>
> 에이다 부스트(AdaBoost)는 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 
> 대표적인 알고리즘이다.</br>

**GBM**

GBM도 에이다 부스트와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는 것이
큰 차이이다. 
경사 하강법은 '반복 수행을 통해 오류를 최소화할 수 있도록 가중치의 업데이트 값을 도출하는 기법'을 
말한다.
일반적으로 GBM은 랜덤 포레스트보다는 예측 성능이 조금 뛰어난 경우가 많다.
그러나 수행 시간이 오래 걸리고, 하이퍼파라미터 튜닝 노력도 더 필요하다.
특히 수행 시간 문제는 GBM이 극복해야 할 중요한 과제이다.
사이킷런의 GradientBoostingClassifier는 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 
수행하므로 멀티 CPU 코어 시스템을 사용하더라도 병렬 처리가 지원되지 않아서 대용량 데이터의 경우
학습에 매우 많은 시간이 필요하다.
반면에 랜덤 포레스트의 경우 상대적으로 빠른 수행 시간을 보장해주기 때문에 더 쉽게 예측 결과를
도출할 수 있다.

> GBM은 과적합에도 강한 뛰어난 예측 성능을 가진 알고리즘이다. </br>
> 하지만, 수행 시간이 오래 걸린다는 단점이 있다. (약한 학습기의 순차적인 예측 오류 보정)


