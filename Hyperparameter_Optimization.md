**그리드 서치**

그리드 서치는 가장 기본적인 하이퍼파라미터 최적화 기법으로, 주어진 하이퍼파라미터를
모두 순회하며 가장 좋은 성능을 내는 값을 찾는다. 모든 경우의 수를 탐색하는 방식이라
시간이 오래 걸린다는 단점이 있다.

**랜덤 서치**

랜덤 서치는 하이퍼파라미터를 무작위로 탐색해 가장 좋은 성능을 내는 값을 찾는 기법이다.
무작위라는 한계 때문에 그리드 서치나 베이지안 최적화에 비해 사용 빈도가 떨어진다.
랜덤 서치는 사이킷런이 제공하는 RandomSearchCV() 메서드로 수행할 수 있다.

>그리드 서치는 한 가지 주요한 단점이 있다. 
튜닝할 하이퍼파라미터의 개수가 많을 경우 최적화 수행 시간이 오래 걸린다.
개별 하이퍼파라미터 값의 범위가 넓거나 학습 데이터가 대용량일 경우에 최적화 시간이 더욱 늘어난다.
XGBoost와 LightGBM은 성능이 매우 뛰어난 알고리즘이지만, 하이퍼파라미터 개수가 다른 알고리즘에 비해서 많다. 
때문에, 그리드 서치 방식으로 최적 하이퍼파라미터를 찾으려면 많은 시간이 소모될 수 있다.
따라서, 그리드 서치 방식보다는 다른 방식을 적용하곤 하는데 대표적으로 베이지안 최적화 기법이 있다.

**베이지안 최적화**

베이지안 최적화는 목적 함수 식을 제대로 알 수 없는 블랙박스 형태의 함수에서 최대 또는 최소 함수 반환
값을 만드는 최적 입력 값을 가능한 적은 시도를 통해 빠르고 효과적으로 찾아주는 방식이다.
베이지안 확률에 기반을 두고 있는 최적화 기법이다.

베이지안 최적화의 두 가지 중요 요소
* 대체 모델 => 획득 함수로부터 최적 함수를 예측할 수 있는 입력 값을 추천 받은 뒤 이를 기반으로 
			  최적 함수 모델을 개선해 나간다.
* 획득 함수 => 개선된 대체 모델을 기반으로 최적 입력 값을 계산한다.

>베이지안 최적화 단계
>Step 1: 최초에는 랜덤하게 하이퍼파라미터들을 샘플링하고 성능 결과를 관측한다.
>Step 2: 관측된 값을 기반으로 대체 모델은 최적 함수를 추정한다.
>Step 3: 추정된 최적 함수를 기반으로 획득 함수는 다음으로 관측할 하이퍼파라미터 값을 계산한다.
>	   획득 함수는 이전의 최적 관측 값보다 더 큰 최댓값을 가질 가능성이 높은 지점을 찾아서 다음에
>	   관측할 하이퍼파라미터를 대체 모델에 전달한다.
>Step 4: 획득 함수로부터 전달된 하이퍼파라미터를 수행하여 관측된 값을 기반으로 대체 모델은 갱신되어
>	   다시 최적 함수를 예측 추정한다.



